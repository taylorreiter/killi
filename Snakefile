RAWDATA_DIR = "inputs/raw"
SAMPLES = ['AW_BI_1_Soil', 'AW_BI_1_Water', 'AW_BI_2_Soil', 'AW_BI_2_Water', 
           'AW_F2_Gill', 'AW_F2_Gut', 'AW_F2_Lung', 'AW_F2_Swab', 
           'AW_M2_Gill', 'AW_M2_Gut', 'AW_M2_Lung', 'AW_M2_Swab']

rule all:
    input:
        'outputs/comp/comp_trim.csv',
        expand("outputs/gather/{sample}.csv", sample = SAMPLES),
        expand('outputs/map_to_megahit/{sample}.flagstat', sample = SAMPLES)

rule cat_samples_R1:
# concatenate lanes together into single sample
    output: 'inputs/cat/{sample}_R1.fq.gz'
    params: indir = RAWDATA_DIR
    shell:'''
    cat {params.indir}/{wildcards.sample}_S*_L00*_R1_001.fastq.gz > {output}
    '''

rule cat_samples_R2:
# concatenate lanes together into single sample
    output: 'inputs/cat/{sample}_R2.fq.gz'
    params: indir = RAWDATA_DIR
    shell:'''
    cat {params.indir}/{wildcards.sample}_S*_L00*_R2_001.fastq.gz > {output}
    '''

rule download_adapters:
    output: "inputs/adapters.fa"
    shell:'''
    # place holder to download all_PE.fa. Generated by concatenating all PE
    # illumina adapters together that ship with trimmomatic.
    '''

rule adapter_trim:
    input:
        r1 = "inputs/cat/{sample}_R1.fq.gz",
        r2 = 'inputs/cat/{sample}_R2.fq.gz',
        adapters = 'inputs/adapters.fa'
    output:
        r1 = 'outputs/trim/{sample}_R1.trim.fq.gz',
        r2 = 'outputs/trim/{sample}_R2.trim.fq.gz',
        o1 = 'outputs/trim/{sample}_o1.trim.fq.gz',
        o2 = 'outputs/trim/{sample}_o2.trim.fq.gz'
    conda: 'env.yml'
    shell:'''
     trimmomatic PE {input.r1} {input.r2} \
             {output.r1} {output.o1} {output.r2} {output.o2} \
             ILLUMINACLIP:{input.adapters}:2:0:15 MINLEN:25  \
             LEADING:2 TRAILING:2 SLIDINGWINDOW:4:2
    '''

rule sourmash_compute_trim:
    input:
        r1 = 'outputs/trim/{sample}_R1.trim.fq.gz',
        r2 = 'outputs/trim/{sample}_R2.trim.fq.gz'
    output: 'outputs/sigs/{sample}_trim.sig'
    conda: 'env.yml'
    shell:'''
    sourmash compute -k 21,31,51 --scaled 2000 --track-abundance \
            --merge {wildcards.sample} -o {output} \
            {input.r1} {input.r2}
    '''

rule sourmash_gather:
    input: 
        sig='outputs/sigs/{sample}_trim.sig',
        killi_sig='outputs/sigs/GCF_000826765.1.sig',
        genbank='../cosmo-kmers/inputs/databases/genbank-d2-k51.sbt.json'
    output: "outputs/gather/{sample}.csv"
    conda: 'env.yml'
    shell:'''
   sourmash gather -o {output} --scaled 2000 -k 51 {input.sig} {input.killi_sig} {input.genbank}
    '''
 
rule download_killifish_genome:
    output: 'inputs/host/GCF_000826765.1.fna.gz'
    shell:'''
    wget -O {output} ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/826/765/GCF_000826765.1_Fundulus_heteroclitus-3.0.2/GCF_000826765.1_Fundulus_heteroclitus-3.0.2_genomic.fna.gz 
    '''

rule calculate_killifish_sig:
    input: 'inputs/host/GCF_000826765.1.fna.gz'
    output: 'outputs/sigs/GCF_000826765.1.sig'
    conda: 'env.yml'
    shell:'''
    sourmash compute -k 21,31,51 --scaled 2000 --track-abundance -o {output} {input}
    '''  

rule remove_host:
    output:
        r1 = 'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        r2 = 'outputs/bbduk/{sample}_R2.nohost.fq.gz',
        killi_r1='outputs/bbduk/{sample}_R1.killi.fq.gz',
        killi_r2='outputs/bbduk/{sample}_R2.killi.fq.gz'
    input: 
        r1 = 'outputs/trim/{sample}_R1.trim.fq.gz',
        r2 = 'outputs/trim/{sample}_R2.trim.fq.gz',
        killi='inputs/host/GCF_000826765.1.fna.gz'
    conda: 'env.yml'
    shell:'''
    bbduk.sh -Xmx64g in={input.r1} in2={input.r2} out={output.r1} out2={output.r2} outm={output.killi_r1} outm2={output.killi_r2} k=31 ref={input.killi}
    '''

rule sourmash_compute_nohost:
    input:
        r1 = 'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        r2 = 'outputs/bbduk/{sample}_R2.nohost.fq.gz'
    output: 'outputs/sigs/{sample}_nohost.sig'
    conda: 'env.yml'
    shell:'''
    sourmash compute -k 21,31,51 --scaled 2000 --track-abundance \
            --merge {wildcards.sample} -o {output} \
            {input.r1} {input.r2}
    '''

rule sourmash_compare:
    input: expand('outputs/sigs/{sample}.sig', sample = SAMPLES)
    output: 'outputs/comp/comp_trim.csv'
    conda: 'env.yml'
    shell:'''
    sourmash compare -k 31 --csv {output} {input}
    '''

rule megahit:
    input:
        r1 = 'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        r2 = 'outputs/bbduk/{sample}_R2.nohost.fq.gz'
    output: 'outputs/megahit/{sample}.contigs.fa'
    conda: 'env.yml'
    params: output_folder = 'outputs/megahit/'
    shell:'''
    # megahit does not allow force overwrite, so each assembly needs to occur
    # in it's own directory.
    megahit -1 {input.r1} -2 {input.r2} --min-contig-len 142 \
        --out-dir {wildcards.sample}_megahit \
        --out-prefix {wildcards.sample}
    # move the final assembly to a folder containing all assemblies
    mv {wildcards.sample}_megahit/{wildcards.sample}.contigs.fa {output}
    # remove the original megahit assembly folder, which is in the main directory.
    rm -rf {wildcards.sample}_megahit
    '''

rule index_megahit:
    input: 'outputs/megahit/{sample}.contigs.fa'
    output: 'outputs/megahit/{sample}.contigs.fa.bwt'
    conda: 'env.yml'
    shell:'''
    bwa index {input}
    '''

rule map_to_megahit:
    input: 
        assembly='outputs/megahit/{sample}.contigs.fa',
        indx='outputs/megahit/{sample}.contigs.fa.bwt',
        r1 = 'outputs/bbduk/{sample}_R1.nohost.fq.gz',
        r2 = 'outputs/bbduk/{sample}_R2.nohost.fq.gz'
    output: 'outputs/map_to_megahit/{sample}.sam'
    conda:'env.yml'
    shell:'''
    bwa mem {input.assembly} {input.r1} {input.r2} > {output}
    '''

rule flagstat_megahit:
    input: 'outputs/map_to_megahit/{sample}.sam'
    output: 'outputs/map_to_megahit/{sample}.flagstat'
    conda:'env.yml'
    shell:'''
    samtools flagstat {input} > {output}
    '''
